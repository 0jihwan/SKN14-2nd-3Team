import streamlit as st
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import matplotlib
import matplotlib.font_manager as fm
font_path = "C:\\Windows\\Fonts\\H2GTRM.TTF" # ìœˆë„ìš°OS í°íŠ¸ê²½ë¡œ
font_prop = fm.FontProperties(fname=font_path)
font_name = font_prop.get_name() # í°íŠ¸ëª…
matplotlib.rc('font', family=font_name)
plt.rc('axes', unicode_minus=False) # matplotlibì´ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ìœ ë‹ˆì½”ë“œ ë§ˆì´ë„ˆìŠ¤ ë¹„í™œì„±í™”, ì•„ìŠ¤í‚¤ì½”ë“œ ë§ˆì´ë„ˆìŠ¤ ì‚¬ìš©)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from utils.preprocessing import preprocess_commerce_data, split_and_scale

# ëª¨ë¸ ê²½ë¡œ ë° threshold ì„¤ì •
model_options = {
    "XGBoost": ("../models/xgboost_model(threshold=0.022).pkl", 0.022),
    "GradientBoosting": ("../models/gb_model(threshold=0.134).pkl", 0.134),
    "RandomForest": ("../models/random_forest_model(threshold=0.285).pkl", 0.285),
    "HistGradientBoosting": ("../models/hgb_model(threshold=0.0004).pkl", 0.0004),
    "Stacking": ("../models/stack_model(threshold=0.065).pkl", 0.065),
    "Voting": ("../models/voting_model(threshold=0.137).pkl", 0.137)
}

st.header("3. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")

# í‰ê°€ ë°©ì‹ ì„ íƒ
view_mode = st.radio("ðŸ§ª í‰ê°€ ë°©ì‹ ì„ íƒ", ["ê°œë³„ ëª¨ë¸ ìƒì„¸ ë³´ê¸°", "ëª¨ë“  ëª¨ë¸ ë¹„êµ"], horizontal=True)

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬
df_raw = pd.read_csv('data/CommerceData.csv')
df_processed = preprocess_commerce_data(df_raw)

exclude = ['NeverOrdered', 'CityTier', 'PreferredPaymentMode', 'Gender',
           'PreferedOrderCat', 'MaritalStatus', 'PreferredLoginDevice']

X_train, X_test, y_train, y_test, scaler = split_and_scale(
    df_processed, target_col='Churn', exclude_cols=exclude
)

if view_mode == "ê°œë³„ ëª¨ë¸ ìƒì„¸ ë³´ê¸°":
    selected_model = st.selectbox("ðŸ” í™•ì¸í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", list(model_options.keys()))
    model_path, threshold = model_options[selected_model]
    model = joblib.load(model_path)

    predict_mode = st.radio("ì˜ˆì¸¡ ë°©ì‹ ì„ íƒ", ["Threshold ì ìš©", "ê¸°ë³¸ ì˜ˆì¸¡"], horizontal=True)

    if predict_mode == "Threshold ì ìš©" and hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
        y_pred = (y_proba >= threshold).astype(int)
        st.info(f"ðŸ“Œ Threshold `{threshold}` ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        y_pred = model.predict(X_test)
        st.info("ðŸ“Œ ê¸°ë³¸ ì˜ˆì¸¡ ê²°ê³¼ìž…ë‹ˆë‹¤.")

    y_true = y_test

    acc = accuracy_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    report = classification_report(y_true, y_pred, output_dict=True)

    st.markdown(f"""
    - ì‚¬ìš© ëª¨ë¸: **{selected_model}**
    - í‰ê°€ ê¸°ì¤€: Accuracy, Recall, Precision, F1 Score
    
    ðŸ“Œ **ì„±ëŠ¥ ê²°ê³¼**
    - Accuracy: `{acc:.4f}`  
    - Recall: `{rec:.4f}`  
    - Precision: `{prec:.4f}`  
    - F1 Score: `{f1:.4f}`
    """)

    metrics = {"Accuracy": acc, "Recall": rec, "Precision": prec, "F1 Score": f1}
    fig, ax = plt.subplots()
    ax.bar(metrics.keys(), metrics.values(), color=["skyblue", "salmon", "lightgreen", "violet"])
    ax.set_ylim(0, 1)
    ax.set_title(f"{selected_model} ì„±ëŠ¥ ì§€í‘œ - {predict_mode}")
    st.pyplot(fig)

    st.subheader("ðŸ“‹ Classification Report")
    accuracy_val = report.pop("accuracy")
    report_df = pd.DataFrame(report).transpose().round(4)
    st.dataframe(report_df)
    st.markdown(f"âœ… **Overall Accuracy**: `{accuracy_val:.4f}`")

elif view_mode == "ëª¨ë“  ëª¨ë¸ ë¹„êµ":
    st.subheader("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (ê¸°ë³¸ vs Threshold)")

    compare_results = []

    for model_name, (path, threshold) in model_options.items():
        model = joblib.load(path)

        # Default predict()
        y_pred_default = model.predict(X_test)

        # Threshold predict_proba()
        if hasattr(model, "predict_proba"):
            y_proba = model.predict_proba(X_test)[:, 1]
            y_pred_thresh = (y_proba >= threshold).astype(int)
        else:
            y_pred_thresh = y_pred_default

        y_true = y_test

        for version, y_pred in zip(["Default", "Threshold"], [y_pred_default, y_pred_thresh]):
            report = classification_report(y_true, y_pred, output_dict=True)
            accuracy = report["accuracy"]
            pos_recall = report["1"]["recall"]

            compare_results.append({
                "Model": model_name,
                "Version": version,
                "Accuracy": accuracy,
                "Positive Recall": pos_recall
            })

    # DataFrame ë³€í™˜
    compare_df = pd.DataFrame(compare_results)
    st.dataframe(compare_df.round(4))